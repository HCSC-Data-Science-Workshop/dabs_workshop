{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9b74890-aed6-4cca-8eaf-d96aba3f23f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Managing the model lifecycle in Unity Catalog\n",
    "\n",
    "One of the primary challenges among data scientists and ML engineers is the absence of a central repository for models, their versions, and the means to manage them throughout their lifecycle.\n",
    "\n",
    "[Models in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html) addresses this challenge and enables members of the data team to:\n",
    "<br>\n",
    "* **Discover** registered models, current aliases in model development, experiment runs, and associated code with a registered model\n",
    "* **Promote** models to different phases of their lifecycle with the use of model aliases\n",
    "* **Tag** models to capture metadata specific to your MLOps process\n",
    "* **Deploy** different versions of a registered model, offering MLOps engineers the ability to deploy and conduct testing of different model versions\n",
    "* **Test** models in an automated fashion\n",
    "* **Document** models throughout their lifecycle\n",
    "* **Secure** access and permission for model registrations, execution, or modifications\n",
    "\n",
    "We will look at how we test and promote a new __Challenger__ model as a candidate to replace an existing __Champion__ model.\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/mlops/advanced/banners/mlflow-uc-end-to-end-advanced-3.png?raw=true\" width=\"1200\">\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=984752964297111&notebook=%2F02-mlops-advanced%2F03_from_notebook_to_models_in_uc&demo_name=mlops-end2end&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fmlops-end2end%2F02-mlops-advanced%2F03_from_notebook_to_models_in_uc&version=1&user_hash=a3692eff9e5299c6a85c26f2dc27b2e2000517102cea778a7cc80efff9afb355\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0f6813d-a3dd-454f-8fce-c8c0cdf545c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## How to use Models in Unity Catalog\n",
    "Typically, data scientists who use MLflow will conduct many experiments, each with a number of runs that track and log metrics and parameters. During this development cycle, they will select the best run within an experiment and register its model to Unity Catalog.  Think of this as **committing** the model to the Unity Catalog, much as you would commit code to a version control system.\n",
    "\n",
    "Unity Catalog proposes free-text model alias, i.e., `Baseline`, `Challenger`, `Champion`, along with tagging.\n",
    "\n",
    "Users with appropriate permissions can create models, modify aliases and tags, use models, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ec6058c-db4b-4c46-8f02-5bfd425f6cf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet mlflow==2.19\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13f81430-0c97-42e0-8e93-1e3b43903b5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup $adv_mlops=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69062519-0a01-494c-b607-d2154e79c254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Programmatically find best run and push model to Unity Catalog for validation\n",
    "\n",
    "We have completed the training runs to find a candidate __Challenger__ model. We'll programatically select the best model from our last ML experiment and register it to Unity Catalog. We can easily do that using MLFlow `search_runs` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d73f5dfe-8467-44e1-9128-6584a9550ab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "model_name = f\"{catalog}.{db}.advanced_mlops_churn\"\n",
    "\n",
    "print(f\"Finding best run from {xp_name} and pushing new model version to {model_name}\")\n",
    "mlflow.set_experiment(f\"{xp_path}/{xp_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00011d60-54bd-4dc2-9e9b-fa28c9569c14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's get our best ml run\n",
    "best_model = mlflow.search_runs(\n",
    "  order_by=[\"metrics.test_f1_score DESC\"],\n",
    "  max_results=1,\n",
    "  filter_string=\"status = 'FINISHED' and run_name='mlops_best_run'\" #filter on mlops_best_run to always use the notebook 02 to have a more predictable demo\n",
    ")\n",
    "# Optional: Load MLflow Experiment as a spark df and see all runs\n",
    "# df = spark.read.format(\"mlflow-experiment\").load(experiment_id)\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45c1ee3c-f73d-4111-9e54-ed0acb6dbef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Once we have our best model, we can now register it to the Unity Catalog Model Registry using its run ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e4293d3-2123-4aa4-b1cc-96b1c99ba4ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Registering model to {catalog}.{db}.advanced_mlops_churn\")\n",
    "\n",
    "# Get the run id from the best model\n",
    "run_id = best_model.iloc[0]['run_id']\n",
    "\n",
    "# Register best model from experiments run to MLflow model registry\n",
    "model_details = mlflow.register_model(f\"runs:/{run_id}/model\", f\"{catalog}.{db}.advanced_mlops_churn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47275212-ba92-40af-8d6c-1b86cdf6c7c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "At this point the model does not yet have any aliases or description that indicates its lifecycle and meta-data/info.  Let's update this information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "586a5e3e-da4b-4eb2-b45f-993e92e4d0fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Give the registered model a description\n",
    "\n",
    "We'll do this for the registered model overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d12b061-b8ab-4682-b33d-a08131c5d6ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "client = MlflowClient()\n",
    "\n",
    "# The main model description, typically done once.\n",
    "client.update_registered_model(\n",
    "  name=model_details.name,\n",
    "  description=\"This model predicts whether a customer will churn using the features in the mlops_churn_training table. It is used to power the Telco Churn Dashboard in DB SQL.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84bb0a17-c70f-4efa-aaef-75d13aa55e5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "And add some more details on the new version we just registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e05d766a-ed77-4a11-8ee4-c9e4431c73d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Provide more details on this specific model version\n",
    "best_score = best_model['metrics.test_f1_score'].values[0]\n",
    "run_name = best_model['tags.mlflow.runName'].values[0]\n",
    "version_desc = f\"This model version has an F1 validation metric of {round(best_score,4)*100}%. Follow the link to its training run for more details.\"\n",
    "\n",
    "client.update_model_version(\n",
    "  name=model_details.name,\n",
    "  version=model_details.version,\n",
    "  description=version_desc\n",
    ")\n",
    "\n",
    "# We can also tag the model version with the F1 score for visibility\n",
    "client.set_model_version_tag(\n",
    "  name=model_details.name,\n",
    "  version=model_details.version,\n",
    "  key=\"f1_score\",\n",
    "  value=f\"{round(best_score,4)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "532000dc-09b2-49da-806d-b91bfaa3d2a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Set the latest model version as the Challenger model\n",
    "\n",
    "We will set this newly registered model version as the __Challenger__ model. Challenger models are candidate models to replace the Champion model, which is the model currently in use.\n",
    "\n",
    "We will use the model's alias to indicate the stage it is at in its lifecycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb7cc191-424d-4d04-a171-ad2402623151",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set this version as the Challenger model, using its model alias\n",
    "client.set_registered_model_alias(\n",
    "  name=f\"{catalog}.{db}.advanced_mlops_churn\",\n",
    "  alias=\"Challenger\",\n",
    "  version=model_details.version\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c88a4f7-83a5-4054-a386-ee9c1b442450",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Now, visually inspect the model versions in Unity Catalog Explorer. You should see the version description and `Challenger` alias applied to the version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6067297-e9f9-4d4e-b685-8298feef6434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Model lineage in Unity Catalog\n",
    "\n",
    "Unity Catalog allows you to track model lineage. Select the model in the Catalog Explorer, click on the latest model version, and click **See lineage graph** in the **Lineage** tab.\n",
    "\n",
    "Unity Catalog captures the upstream featurization logic in the form of the feature table used to train the model and the feature function used to calculate features. It also traces back to the notebook where the model was trained. You can navigate to all these assets using the link displayed on the screen. This allows you to have traceability on where the model got its features from and lets you perform impact analysis.\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/mlops/advanced/03_model_lineage.png?raw=true\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fafbbff-3897-46c7-8a1e-da56169a24d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next: Validation of the Challenger model\n",
    "\n",
    "At this point, with the __Challenger__ model registered, we would like to validate the model. The validation steps are implemented in a notebook, so the validation process can be automated as part of a Databricks Workflow job.\n",
    "\n",
    "If the model passes all the tests, it'll be promoted to `Champion`.\n",
    "\n",
    "Next: Find out how the model is being tested before being promoted as `Champion` [using the model validation notebook]($./04_challenger_validation)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "(Clone) 03_from_notebook_to_models_in_uc",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

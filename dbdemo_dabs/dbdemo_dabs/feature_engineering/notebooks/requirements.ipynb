{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6db5a116-9c7f-4988-90af-f04a39d23590",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "install dbldatagen for generating the synthetic data"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install dbldatagen\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2b8e54a-7036-495d-9c59-a759e83e49b6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "define the working directory"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "notebook_path =  '/Workspace/' + os.path.dirname(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get())\n",
    "%cd $notebook_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7040781d-da34-4099-a814-9f6726eaac74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "#remove warnings for nicer display\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.ERROR)\n",
    "from mlflow import MlflowClient\n",
    "from pyspark.sql.functions import expr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbc2b7bc-7517-41ca-8c3e-81b95a6497d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# A Hive-registered Delta table containing the input data.\n",
    "dbutils.widgets.text(\n",
    "    \"bronze_table_name\",\n",
    "    \"main.dbdemos_mlops.advanced_churn_bronze_customers\",\n",
    "    label=\"Raw Bronze Table Name\",\n",
    ")\n",
    "# Feature table to store the computed features.\n",
    "dbutils.widgets.text(\n",
    "    \"inference_table_name\",\n",
    "    \"dev.koeppen_dabs_demo.advanced_churn_inference_table\",\n",
    "    label=\"Inference Table\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68b06c0f-f6d4-42a8-a401-e60b346a727b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_table_name = dbutils.widgets.get(\"bronze_table_name\")\n",
    "inference_table_name = dbutils.widgets.get(\"inference_table_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4f02579-1112-4d93-9312-dc8d58da940d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44062d23-569d-402c-8338-93af5298612c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "If the Bronze Data doesn't already exist, create it from the csv"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "if not spark.catalog.tableExists(bronze_table_name):\n",
    "  import requests\n",
    "  from io import StringIO\n",
    "  #Dataset under apache license: https://github.com/IBM/telco-customer-churn-on-icp4d/blob/master/LICENSE\n",
    "  csv = requests.get(\"https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv\").text\n",
    "  df = pd.read_csv(StringIO(csv), sep=\",\")\n",
    "  def cleanup_column(pdf):\n",
    "    # Clean up column names\n",
    "    pdf.columns = [re.sub(r'(?<!^)(?=[A-Z])', '_', name).lower().replace(\"__\", \"_\") for name in pdf.columns]\n",
    "    pdf.columns = [re.sub(r'[\\(\\)]', '', name).lower() for name in pdf.columns]\n",
    "    pdf.columns = [re.sub(r'[ -]', '_', name).lower() for name in pdf.columns]\n",
    "    return pdf.rename(columns = {'streaming_t_v': 'streaming_tv', 'customer_i_d': 'customer_id'})\n",
    "\n",
    "  df = cleanup_column(df)\n",
    "  print(f\"creating `{bronze_table_name}` raw table\")\n",
    "  spark.createDataFrame(df).write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(bronze_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ada56e4-67a4-47ea-ac63-533925b6343d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "generate synthetic data for inference"
    }
   },
   "outputs": [],
   "source": [
    "def generate_synthetic(inference_table, drift_type=\"label_drift\"):\n",
    "  import dbldatagen as dg\n",
    "  import pyspark.sql.types\n",
    "  from databricks.feature_engineering import FeatureEngineeringClient\n",
    "  import pyspark.sql.functions as F\n",
    "  from datetime import datetime, timedelta\n",
    "  # Column definitions are stubs only - modify to generate correct data  \n",
    "  #\n",
    "  generation_spec = (\n",
    "      dg.DataGenerator(sparkSession=spark, \n",
    "                      name='synthetic_data', \n",
    "                      rows=50000,\n",
    "                      random=True,\n",
    "                      )\n",
    "      .withColumn('customer_id', 'string', template=r'dddd-AAAA')\n",
    "      .withColumn('transaction_ts', 'timestamp', begin=(datetime.now() + timedelta(days=-30)), end=(datetime.now() + timedelta(days=-1)), interval=\"1 hour\")\n",
    "      .withColumn('gender', 'string', values=['Female', 'Male'], random=True, weights=[0.5, 0.5])\n",
    "      .withColumn('senior_citizen', 'string', values=['No', 'Yes'], random=True, weights=[0.85, 0.15])\n",
    "      .withColumn('partner', 'string', values=['No', 'Yes'], random=True, weights=[0.5, 0.5])\n",
    "      .withColumn('dependents', 'string', values=['No', 'Yes'], random=True, weights=[0.7, 0.3])\n",
    "      .withColumn('tenure', 'double', minValue=0.0, maxValue=72.0, step=1.0)\n",
    "      .withColumn('phone_service', values=['No', 'Yes'], random=True, weights=[0.9, 0.1])\n",
    "      .withColumn('multiple_lines', 'string', values=['No', 'Yes'], random=True, weights=[0.5, 0.5])\n",
    "      .withColumn('internet_service', 'string', values=['Fiber optic', 'DSL', 'No'], random=True, weights=[0.5, 0.3, 0.2])\n",
    "      .withColumn('online_security', 'string', values=['No', 'Yes'], random=True, weights=[0.5, 0.5])\n",
    "      .withColumn('online_backup', 'string', values=['No', 'Yes'], random=True, weights=[0.5, 0.5])\n",
    "      .withColumn('device_protection', 'string', values=['No', 'Yes'], random=True, weights=[0.5, 0.5])\n",
    "      .withColumn('tech_support', 'string', values=['No', 'Yes'], random=True, weights=[0.5, 0.5])\n",
    "      .withColumn('streaming_tv', 'string', values=['No', 'Yes', 'No internet service'], random=True, weights=[0.4, 0.4, 0.2])\n",
    "      .withColumn('streaming_movies', 'string', values=['No', 'Yes', 'No internet service'], random=True, weights=[0.4, 0.4, 0.2])\n",
    "      .withColumn('contract', 'string', values=['Month-to-month', 'One year','Two year'], random=True, weights=[0.5, 0.25, 0.25])\n",
    "      .withColumn('paperless_billing', 'string', values=['No', 'Yes'], random=True, weights=[0.6, 0.4])\n",
    "      .withColumn('payment_method', 'string', values=['Credit card (automatic)', 'Mailed check',\n",
    "  'Bank transfer (automatic)', 'Electronic check'], weights=[0.2, 0.2, 0.2, 0.4])\n",
    "      .withColumn('monthly_charges', 'double', minValue=18.0, maxValue=118.0, step=0.5)\n",
    "      .withColumn('total_charges', 'double', minValue=0.0, maxValue=8684.0, step=20)\n",
    "      .withColumn('num_optional_services', 'double', minValue=0.0, maxValue=6.0, step=1)\n",
    "      .withColumn('avg_price_increase', 'float', minValue=-19.0, maxValue=130.0, step=20)\n",
    "      .withColumn('churn', 'string', values=['Yes'], random=True)\n",
    "      )\n",
    "\n",
    "\n",
    "  # Generate Synthetic Data\n",
    "  df_synthetic_data = generation_spec.build()\n",
    "  df_synthetic_data.write.mode(\"overwrite\").saveAsTable(inference_table)\n",
    "\n",
    "generate_synthetic(inference_table=inference_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce114a4-ed06-4ada-a00f-07e4b53f7186",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3369811628431739,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "requirements",
   "widgets": {
    "bronze_table_name": {
     "currentValue": "main.dbdemos_mlops.advanced_churn_bronze_customers",
     "nuid": "f15615fe-a353-4066-a963-144e9e9eefa7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "main.dbdemos_mlops.advanced_churn_bronze_customers",
      "label": "Raw Bronze Table Name",
      "name": "bronze_table_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "main.dbdemos_mlops.advanced_churn_bronze_customers",
      "label": "Raw Bronze Table Name",
      "name": "bronze_table_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "inference_table_name": {
     "currentValue": "dev.koeppen_dabs_demo.advanced_churn_inference_table",
     "nuid": "c5cefd7b-757e-4747-88c4-568f339ae4c5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev.koeppen_dabs_demo.advanced_churn_inference_table",
      "label": "Inference Table",
      "name": "inference_table_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dev.koeppen_dabs_demo.advanced_churn_inference_table",
      "label": "Inference Table",
      "name": "inference_table_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
